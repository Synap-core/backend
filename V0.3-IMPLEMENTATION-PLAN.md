# üöÄ V0.3 Implementation Plan - Detailed Execution

**Status**: ‚úÖ APPROVED - Ready to Execute  
**Timeline**: 3 weeks  
**Start Date**: TBD  
**Team**: 1-2 engineers  

---

## üìã Pre-Implementation Research (Day 0)

### Research Task #1: Neon Extensions Support

**Question**: Does Neon support TimescaleDB extension?

**Action**:
```bash
# Check Neon documentation
open https://neon.tech/docs/extensions/extensions-list

# Test on our Neon instance
psql $DATABASE_URL -c "CREATE EXTENSION IF NOT EXISTS timescaledb;"
```

**Expected Results**:
- ‚úÖ **If supported**: Use TimescaleDB on Neon (same database)
- ‚ùå **If not**: Deploy separate TimescaleDB instance

**Decision Matrix**:
```
Neon supports TimescaleDB ‚Üí Use TimescaleDB on Neon
Neon doesn't support ‚Üí Options:
  A. Use EventStoreDB (separate service)
  B. Use separate TimescaleDB instance (Railway, Supabase)
  C. Keep PostgreSQL with partitioning (acceptable to 10M events)
```

---

### Research Task #2: Cloudflare R2 Setup

**Question**: How to use Cloudflare R2 with Node.js?

**Resources**:
- Cloudflare R2 Docs: https://developers.cloudflare.com/r2/
- S3 SDK Compatibility: https://developers.cloudflare.com/r2/api/s3/

**Key Findings** (based on documentation):

1. **R2 is S3-compatible**
   - Use AWS SDK for S3
   - Just change endpoint URL
   - Same API, lower cost

2. **Setup Steps**:
```bash
# 1. Create R2 bucket via Cloudflare dashboard
# 2. Generate API token
# 3. Get account ID and bucket name
```

3. **Node.js Implementation**:
```typescript
import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';

const r2 = new S3Client({
  region: 'auto',
  endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: R2_ACCESS_KEY_ID,
    secretAccessKey: R2_SECRET_ACCESS_KEY,
  },
});

// Upload
await r2.send(new PutObjectCommand({
  Bucket: 'synap-storage',
  Key: `users/${userId}/notes/${entityId}.md`,
  Body: content,
  ContentType: 'text/markdown',
}));

// Download
const response = await r2.send(new GetObjectCommand({
  Bucket: 'synap-storage',
  Key: `users/${userId}/notes/${entityId}.md`,
}));
const content = await response.Body.transformToString();

// Signed URL (1 hour expiry)
const url = await getSignedUrl(r2, new GetObjectCommand({
  Bucket: 'synap-storage',
  Key: key,
}), { expiresIn: 3600 });
```

**Pricing** (Cloudflare R2):
- Storage: $0.015/GB/month (15x cheaper than PostgreSQL!)
- Class A ops (writes): $4.50 per million
- Class B ops (reads): $0.36 per million
- Egress: $0 (FREE!)

**Recommendation**: ‚úÖ Use R2 (not S3)
- 15x cheaper than PostgreSQL
- 10x cheaper than S3
- Zero egress fees
- S3-compatible API

---

### Research Task #3: EventStoreDB Options

**Question**: How to deploy EventStoreDB?

**Options**:

1. **EventStore Cloud** (Managed)
   - URL: https://www.eventstore.com/event-store-cloud
   - Pricing: ~$99/month minimum
   - Pros: Fully managed, auto-scaling
   - Cons: Cost

2. **Self-Hosted (Docker)**
   - Free (only infrastructure cost)
   - Deploy on: Railway, Render, Fly.io
   - Pros: Cheap, full control
   - Cons: Ops overhead

3. **Don't Use EventStoreDB** (Phase 1)
   - Use TimescaleDB instead
   - Migrate to EventStoreDB later if needed

**Recommendation**: **Start with TimescaleDB, evaluate EventStoreDB for V1.0**

---

## üóìÔ∏è Week 1: R2 Storage Migration

### Day 1 (Monday): R2 Setup & Client Implementation

#### Morning: Infrastructure Setup

**Tasks**:
1. ‚úÖ Create Cloudflare R2 bucket
2. ‚úÖ Generate API tokens
3. ‚úÖ Configure environment variables

**Steps**:
```bash
# 1. Go to Cloudflare Dashboard
open https://dash.cloudflare.com/

# 2. Navigate to R2
# 3. Create bucket: "synap-storage"
# 4. Generate R2 API token
# 5. Copy: Account ID, Access Key, Secret Key

# 6. Add to .env
cat >> .env << EOF
R2_ACCOUNT_ID=your-account-id
R2_ACCESS_KEY_ID=your-access-key
R2_SECRET_ACCESS_KEY=your-secret-key
R2_BUCKET_NAME=synap-storage
R2_PUBLIC_URL=https://synap-storage.r2.dev
EOF
```

**Deliverable**: R2 bucket created, credentials configured

---

#### Afternoon: R2 Client Implementation

**Tasks**:
1. ‚úÖ Install AWS SDK
2. ‚úÖ Create R2Storage class
3. ‚úÖ Write unit tests

**Code**:
```bash
# Install dependencies
pnpm add @aws-sdk/client-s3 @aws-sdk/s3-request-presigner --filter @synap/storage
```

```typescript
// packages/storage/package.json (NEW)
{
  "name": "@synap/storage",
  "version": "1.0.0",
  "main": "./src/index.ts",
  "dependencies": {
    "@aws-sdk/client-s3": "^3.470.0",
    "@aws-sdk/s3-request-presigner": "^3.470.0",
    "crypto": "^1.0.1"
  }
}

// packages/storage/src/r2.ts (NEW)
import {
  S3Client,
  PutObjectCommand,
  GetObjectCommand,
  DeleteObjectCommand,
  HeadObjectCommand,
} from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { createHash } from 'crypto';

export interface FileMetadata {
  url: string;
  path: string;
  size: number;
  checksum: string;
  uploadedAt: Date;
}

export interface UploadOptions {
  contentType?: string;
  metadata?: Record<string, string>;
}

export class R2Storage {
  private client: S3Client;
  private bucketName: string;
  private publicUrl: string;

  constructor(config: {
    accountId: string;
    accessKeyId: string;
    secretAccessKey: string;
    bucketName: string;
    publicUrl?: string;
  }) {
    this.client = new S3Client({
      region: 'auto',
      endpoint: `https://${config.accountId}.r2.cloudflarestorage.com`,
      credentials: {
        accessKeyId: config.accessKeyId,
        secretAccessKey: config.secretAccessKey,
      },
    });
    
    this.bucketName = config.bucketName;
    this.publicUrl = config.publicUrl || `https://${config.bucketName}.r2.dev`;
  }

  /**
   * Upload file to R2
   */
  async upload(
    key: string,
    content: string | Buffer,
    options?: UploadOptions
  ): Promise<FileMetadata> {
    const body = typeof content === 'string' ? Buffer.from(content, 'utf-8') : content;
    const checksum = this.calculateChecksum(body);
    
    await this.client.send(new PutObjectCommand({
      Bucket: this.bucketName,
      Key: key,
      Body: body,
      ContentType: options?.contentType || 'application/octet-stream',
      Metadata: options?.metadata,
      ChecksumSHA256: checksum,
    }));

    return {
      url: `${this.publicUrl}/${key}`,
      path: key,
      size: body.length,
      checksum: `sha256:${checksum}`,
      uploadedAt: new Date(),
    };
  }

  /**
   * Download file from R2
   */
  async download(key: string): Promise<string> {
    const response = await this.client.send(new GetObjectCommand({
      Bucket: this.bucketName,
      Key: key,
    }));

    if (!response.Body) {
      throw new Error(`File not found: ${key}`);
    }

    return await response.Body.transformToString();
  }

  /**
   * Download as buffer (for binary files)
   */
  async downloadBuffer(key: string): Promise<Buffer> {
    const response = await this.client.send(new GetObjectCommand({
      Bucket: this.bucketName,
      Key: key,
    }));

    if (!response.Body) {
      throw new Error(`File not found: ${key}`);
    }

    const chunks: Uint8Array[] = [];
    for await (const chunk of response.Body) {
      chunks.push(chunk);
    }
    
    return Buffer.concat(chunks);
  }

  /**
   * Delete file from R2
   */
  async delete(key: string): Promise<void> {
    await this.client.send(new DeleteObjectCommand({
      Bucket: this.bucketName,
      Key: key,
    }));
  }

  /**
   * Check if file exists
   */
  async exists(key: string): Promise<boolean> {
    try {
      await this.client.send(new HeadObjectCommand({
        Bucket: this.bucketName,
        Key: key,
      }));
      return true;
    } catch (error) {
      return false;
    }
  }

  /**
   * Get metadata without downloading
   */
  async getMetadata(key: string): Promise<{
    size: number;
    lastModified: Date;
    contentType: string;
  }> {
    const response = await this.client.send(new HeadObjectCommand({
      Bucket: this.bucketName,
      Key: key,
    }));

    return {
      size: response.ContentLength || 0,
      lastModified: response.LastModified || new Date(),
      contentType: response.ContentType || 'application/octet-stream',
    };
  }

  /**
   * Generate signed URL (for temporary access)
   */
  async getSignedUrl(key: string, expiresIn: number = 3600): Promise<string> {
    const command = new GetObjectCommand({
      Bucket: this.bucketName,
      Key: key,
    });

    return await getSignedUrl(this.client, command, { expiresIn });
  }

  /**
   * Calculate SHA256 checksum
   */
  private calculateChecksum(data: Buffer): string {
    return createHash('sha256').update(data).digest('base64');
  }

  /**
   * Build file path for user entity
   */
  static buildPath(userId: string, entityType: string, entityId: string, extension: string = 'md'): string {
    return `users/${userId}/${entityType}s/${entityId}.${extension}`;
  }
}

// Export singleton instance
export const r2 = new R2Storage({
  accountId: process.env.R2_ACCOUNT_ID!,
  accessKeyId: process.env.R2_ACCESS_KEY_ID!,
  secretAccessKey: process.env.R2_SECRET_ACCESS_KEY!,
  bucketName: process.env.R2_BUCKET_NAME!,
  publicUrl: process.env.R2_PUBLIC_URL,
});
```

**Tests**:
```typescript
// packages/storage/tests/r2.test.ts
import { describe, it, expect } from 'vitest';
import { R2Storage } from '../src/r2';

describe('R2Storage', () => {
  const r2 = new R2Storage({
    accountId: process.env.R2_ACCOUNT_ID!,
    accessKeyId: process.env.R2_ACCESS_KEY_ID!,
    secretAccessKey: process.env.R2_SECRET_ACCESS_KEY!,
    bucketName: 'test-bucket',
  });

  it('should upload and download file', async () => {
    const testContent = 'Test note content';
    const key = 'test/note.md';

    // Upload
    const metadata = await r2.upload(key, testContent, {
      contentType: 'text/markdown',
    });

    expect(metadata.url).toContain(key);
    expect(metadata.size).toBe(Buffer.byteLength(testContent));
    expect(metadata.checksum).toMatch(/^sha256:/);

    // Download
    const downloaded = await r2.download(key);
    expect(downloaded).toBe(testContent);

    // Cleanup
    await r2.delete(key);
  });

  it('should generate signed URL', async () => {
    const url = await r2.getSignedUrl('test/note.md', 3600);
    expect(url).toContain('X-Amz-Signature');
    expect(url).toContain('X-Amz-Expires=3600');
  });

  it('should calculate checksum correctly', async () => {
    const content = 'Test';
    const result = await r2.upload('test.md', content);
    
    // Verify checksum is deterministic
    const result2 = await r2.upload('test2.md', content);
    expect(result.checksum).toBe(result2.checksum);
    
    await r2.delete('test.md');
    await r2.delete('test2.md');
  });
});
```

**Time**: 4 hours  
**Deliverable**: R2Storage class ready ‚úÖ

---

### Day 2 (Tuesday): Database Schema Updates

#### Morning: Add File Reference Fields

**Task**: Update entities schema

**Code**:
```typescript
// packages/database/src/schema/entities.ts
if (isPostgres) {
  entities = pgTable('entities', {
    // ... existing fields ...
    
    // ‚úÖ NEW: File reference fields
    fileUrl: text('file_url'),
    filePath: text('file_path'),
    fileSize: integer('file_size'),
    fileType: text('file_type'),
    checksum: text('checksum'),
  });
}
```

**Migration**:
```sql
-- packages/database/migrations-pg/0003_add_file_references.sql
ALTER TABLE entities 
  ADD COLUMN file_url TEXT,
  ADD COLUMN file_path TEXT,
  ADD COLUMN file_size INTEGER,
  ADD COLUMN file_type TEXT CHECK (file_type IN ('markdown', 'pdf', 'audio', 'video', 'image')),
  ADD COLUMN checksum TEXT;

CREATE INDEX idx_entities_checksum ON entities(checksum) WHERE checksum IS NOT NULL;
```

**Apply**:
```bash
psql $DATABASE_URL < packages/database/migrations-pg/0003_add_file_references.sql
```

**Time**: 2 hours

---

#### Afternoon: Create entity_vectors Table

**Task**: Separate embedding storage

**Code**:
```sql
-- packages/database/migrations-pg/0004_create_entity_vectors.sql
CREATE TABLE entity_vectors (
  entity_id UUID PRIMARY KEY REFERENCES entities(id) ON DELETE CASCADE,
  user_id TEXT NOT NULL,
  
  -- Embedding
  embedding vector(1536),
  embedding_model TEXT DEFAULT 'text-embedding-3-small',
  
  -- Denormalized for search
  entity_type TEXT,
  title TEXT,
  preview TEXT,
  
  -- Timestamps
  indexed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- HNSW index for similarity search
CREATE INDEX idx_entity_vectors_embedding ON entity_vectors 
  USING hnsw (embedding vector_cosine_ops);

-- User isolation
CREATE INDEX idx_entity_vectors_user ON entity_vectors(user_id);
```

**Apply**:
```bash
psql $DATABASE_URL < packages/database/migrations-pg/0004_create_entity_vectors.sql
```

**Time**: 2 hours  
**Deliverable**: Schema updated ‚úÖ

---

### Day 3 (Wednesday): Dual-Write Implementation

#### Task: Write to BOTH old system AND R2

**Purpose**: Validation period (no data loss)

**Code**:
```typescript
// packages/api/src/routers/notes.ts
.mutation(async ({ ctx, input }) => {
  const userId = requireUserId(ctx.userId);
  const entityId = randomUUID();
  
  // ‚úÖ NEW: Upload to R2
  const file = await r2.upload(
    R2Storage.buildPath(userId, 'note', entityId),
    input.content,
    { contentType: 'text/markdown' }
  );
  
  // ‚úÖ OLD: Still write to content_blocks (for now)
  await ctx.db.insert(contentBlocks).values({
    entityId,
    content: input.content,  // Temporary duplication
  });
  
  // ‚úÖ NEW: Write entity with file reference
  await ctx.db.insert(entities).values({
    id: entityId,
    userId,
    type: 'note',
    fileUrl: file.url,
    filePath: file.path,
    fileSize: file.size,
    fileType: 'markdown',
    checksum: file.checksum,
  });
  
  // Log for monitoring
  console.log('‚úÖ Dual-write: R2 + PostgreSQL', { entityId, fileUrl: file.url });
  
  return { entityId, fileUrl: file.url };
});
```

**Monitoring**:
```typescript
// Verify both writes succeeded
setInterval(async () => {
  const entities = await db.select().from(entities).limit(100);
  
  for (const entity of entities) {
    if (entity.fileUrl) {
      // Check R2 file exists
      const r2Exists = await r2.exists(entity.filePath);
      
      // Check PostgreSQL content exists
      const pgContent = await db.select()
        .from(contentBlocks)
        .where(eq(contentBlocks.entityId, entity.id));
      
      if (!r2Exists || !pgContent) {
        console.error('‚ùå Sync mismatch', { entityId: entity.id });
      }
    }
  }
}, 60000); // Every minute
```

**Time**: 4 hours  
**Deliverable**: Dual-write active, monitoring in place ‚úÖ

---

### Day 4 (Thursday): Content Migration to R2

#### Task: Migrate existing content from PostgreSQL to R2

**Script**:
```typescript
// scripts/migrate-content-to-r2.ts
import { db } from '@synap/database';
import { r2, R2Storage } from '@synap/storage';
import { entities, contentBlocks } from '@synap/database';
import { eq, isNotNull } from 'drizzle-orm';

async function migrateContentToR2() {
  console.log('üöÄ Starting content migration to R2...\n');
  
  // Get all entities with content
  const entitiesToMigrate = await db.select({
    entity: entities,
    content: contentBlocks,
  })
  .from(entities)
  .leftJoin(contentBlocks, eq(entities.id, contentBlocks.entityId))
  .where(isNotNull(contentBlocks.content));

  console.log(`üìä Found ${entitiesToMigrate.length} entities to migrate\n`);

  let migrated = 0;
  let failed = 0;
  const errors: any[] = [];

  for (const item of entitiesToMigrate) {
    const { entity, content } = item;
    
    if (!content || !content.content) {
      console.log(`‚è≠Ô∏è  Skipping ${entity.id} (no content)`);
      continue;
    }

    try {
      // Build R2 path
      const filePath = R2Storage.buildPath(
        entity.userId,
        entity.type,
        entity.id,
        'md'
      );

      // Upload to R2
      const fileMetadata = await r2.upload(
        filePath,
        content.content,
        { contentType: 'text/markdown' }
      );

      // Update entities table with file reference
      await db.update(entities)
        .set({
          fileUrl: fileMetadata.url,
          filePath: fileMetadata.path,
          fileSize: fileMetadata.size,
          fileType: 'markdown',
          checksum: fileMetadata.checksum,
        })
        .where(eq(entities.id, entity.id));

      migrated++;
      console.log(`‚úÖ ${migrated}/${entitiesToMigrate.length} - Migrated ${entity.id}`);

      // Rate limiting (R2 has limits)
      if (migrated % 100 === 0) {
        console.log('‚è∏Ô∏è  Pausing 5 seconds...');
        await new Promise(resolve => setTimeout(resolve, 5000));
      }

    } catch (error) {
      failed++;
      errors.push({ entityId: entity.id, error });
      console.error(`‚ùå Failed to migrate ${entity.id}:`, error);
    }
  }

  console.log('\n' + '='.repeat(50));
  console.log('üìä MIGRATION SUMMARY');
  console.log('='.repeat(50));
  console.log(`‚úÖ Migrated: ${migrated}`);
  console.log(`‚ùå Failed: ${failed}`);
  
  if (errors.length > 0) {
    console.log('\n‚ùå ERRORS:');
    errors.forEach(e => console.log(`  - ${e.entityId}: ${e.error.message}`));
  }
  
  console.log('\n‚úÖ Migration complete!\n');
}

// Run migration
migrateContentToR2()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('üí• Migration failed:', error);
    process.exit(1);
  });
```

**Run**:
```bash
# Dry run first (with limit)
tsx scripts/migrate-content-to-r2.ts --dry-run --limit=10

# Full migration
tsx scripts/migrate-content-to-r2.ts

# Verify
tsx scripts/verify-r2-migration.ts
```

**Time**: 8 hours (includes monitoring)  
**Deliverable**: All content on R2 ‚úÖ

---

### Day 5 (Friday): Verification & Cleanup

#### Morning: Verification

**Script**:
```typescript
// scripts/verify-r2-migration.ts
async function verifyMigration() {
  const allEntities = await db.select().from(entities);
  
  let verified = 0;
  let missing = 0;
  
  for (const entity of allEntities) {
    if (!entity.fileUrl) {
      console.log(`‚è≠Ô∏è  ${entity.id} - No file`);
      continue;
    }
    
    // Check R2 file exists
    const exists = await r2.exists(entity.filePath);
    
    if (exists) {
      // Verify checksum
      const content = await r2.download(entity.filePath);
      const checksum = calculateChecksum(content);
      
      if (`sha256:${checksum}` === entity.checksum) {
        verified++;
        console.log(`‚úÖ ${entity.id} - Verified`);
      } else {
        console.error(`‚ùå ${entity.id} - Checksum mismatch`);
        missing++;
      }
    } else {
      console.error(`‚ùå ${entity.id} - File missing on R2`);
      missing++;
    }
  }
  
  console.log(`\n‚úÖ Verified: ${verified}`);
  console.log(`‚ùå Missing: ${missing}`);
  
  return missing === 0;
}
```

**Time**: 2 hours

---

#### Afternoon: Drop content_blocks Table

**Steps**:
```sql
-- 1. Backup first
pg_dump $DATABASE_URL --table=content_blocks > backup_content_blocks.sql

-- 2. Drop table
DROP TABLE content_blocks CASCADE;

-- 3. Drop embedding column from entities (if exists)
ALTER TABLE entities DROP COLUMN IF EXISTS embedding;
```

**Verify**:
```bash
# Verify API still works
curl http://localhost:3000/trpc/notes.search?query=test

# Should fetch from R2, not content_blocks
```

**Time**: 2 hours  
**Deliverable**: content_blocks table removed ‚úÖ

---

## üóìÔ∏è Week 2: TimescaleDB Migration

### Day 6 (Monday): TimescaleDB Research & Decision

#### Task: Determine if Neon supports TimescaleDB

**Investigation**:
```bash
# Test on Neon
psql $DATABASE_URL << EOF
CREATE EXTENSION IF NOT EXISTS timescaledb;
SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';
EOF
```

**Possible Outcomes**:

**Outcome A**: ‚úÖ Neon supports TimescaleDB
```
Action: Use TimescaleDB on Neon
Cost: $0 extra
Complexity: Low
```

**Outcome B**: ‚ùå Neon doesn't support
```
Options:
1. Deploy separate TimescaleDB (Railway: ~$5/month)
2. Use Supabase (has TimescaleDB support)
3. Use EventStoreDB Cloud ($99/month)
4. Keep PostgreSQL with partitioning (acceptable)

Recommendation: Railway TimescaleDB ($5/month)
```

**Based on Research** (Neon docs):
- Neon supports: pgvector, pg_stat_statements, uuid-ossp, etc.
- TimescaleDB: **NOT listed** in supported extensions (as of 2024)

**Decision**: Deploy separate TimescaleDB instance on Railway

**Time**: 4 hours (research + setup)

---

### Day 7 (Tuesday): TimescaleDB Setup

#### If Using Railway TimescaleDB:

**Steps**:
```bash
# 1. Create Railway account
open https://railway.app

# 2. New Project ‚Üí Add PostgreSQL
# 3. Connect to database
railway login
railway link

# 4. Get connection string
railway variables

# 5. Install TimescaleDB extension
psql $RAILWAY_DATABASE_URL << EOF
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;
SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';
EOF

# 6. Add to .env
echo "TIMESCALE_URL=$RAILWAY_DATABASE_URL" >> .env
```

**Create Hypertable**:
```sql
-- packages/database/migrations-timescale/0001_create_events_hypertable.sql
CREATE TABLE events (
  id UUID DEFAULT gen_random_uuid(),
  timestamp TIMESTAMPTZ NOT NULL,
  
  -- Aggregate reference
  aggregate_id UUID NOT NULL,
  aggregate_type TEXT NOT NULL,
  
  -- Event classification
  event_type TEXT NOT NULL,
  
  -- Ownership
  user_id TEXT NOT NULL,
  
  -- Data (deltas only)
  data JSONB NOT NULL,
  metadata JSONB,
  
  -- Event sourcing
  version INTEGER NOT NULL,
  causation_id UUID,
  correlation_id UUID,
  
  -- Source
  source TEXT NOT NULL DEFAULT 'api'
);

-- Convert to hypertable
SELECT create_hypertable('events', 'timestamp', if_not_exists => TRUE);

-- Compression policy
ALTER TABLE events SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'user_id, aggregate_id',
  timescaledb.compress_orderby = 'timestamp DESC'
);

SELECT add_compression_policy('events', INTERVAL '7 days');

-- Retention policy
SELECT add_retention_policy('events', INTERVAL '2 years');

-- Indexes
CREATE INDEX idx_events_aggregate ON events(aggregate_id, timestamp DESC);
CREATE INDEX idx_events_user ON events(user_id, timestamp DESC);
CREATE INDEX idx_events_type ON events(event_type);
CREATE INDEX idx_events_correlation ON events(correlation_id) WHERE correlation_id IS NOT NULL;

-- Continuous aggregates
CREATE MATERIALIZED VIEW events_daily
WITH (timescaledb.continuous) AS
SELECT
  time_bucket('1 day', timestamp) AS day,
  user_id,
  event_type,
  count(*) as event_count
FROM events
GROUP BY day, user_id, event_type
WITH NO DATA;

SELECT add_continuous_aggregate_policy('events_daily',
  start_offset => INTERVAL '1 month',
  end_offset => INTERVAL '1 day',
  schedule_interval => INTERVAL '1 hour');
```

**Time**: 4 hours  
**Deliverable**: TimescaleDB ready ‚úÖ

---

### Day 8 (Wednesday): Event Repository Implementation

**Task**: Create abstraction for event operations

**Code**:
```typescript
// packages/database/src/repositories/event-repository.ts
import { Pool } from '@neondatabase/serverless';
import { EventType, AggregateType, EventSource } from '@synap/types';

export interface AppendEventData {
  aggregateId: string;
  aggregateType: AggregateType;
  eventType: EventType;
  userId: string;
  data: Record<string, unknown>;
  metadata?: Record<string, unknown>;
  version: number;
  causationId?: string;
  correlationId?: string;
  source?: EventSource;
}

export interface EventRecord {
  id: string;
  timestamp: Date;
  aggregateId: string;
  aggregateType: string;
  eventType: string;
  userId: string;
  data: Record<string, unknown>;
  metadata?: Record<string, unknown>;
  version: number;
  causationId?: string;
  correlationId?: string;
  source: string;
}

export class EventRepository {
  constructor(private pool: Pool) {}

  /**
   * Append event to stream
   */
  async append(event: AppendEventData): Promise<EventRecord> {
    // Optimistic concurrency check
    const currentVersion = await this.getAggregateVersion(event.aggregateId);
    
    if (currentVersion !== null && event.version !== currentVersion + 1) {
      throw new Error(
        `Concurrency conflict: expected version ${currentVersion + 1}, got ${event.version}`
      );
    }

    const result = await this.pool.query(`
      INSERT INTO events (
        aggregate_id,
        aggregate_type,
        event_type,
        user_id,
        data,
        metadata,
        version,
        causation_id,
        correlation_id,
        source,
        timestamp
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW())
      RETURNING *
    `, [
      event.aggregateId,
      event.aggregateType,
      event.eventType,
      event.userId,
      JSON.stringify(event.data),
      event.metadata ? JSON.stringify(event.metadata) : null,
      event.version,
      event.causationId || null,
      event.correlationId || null,
      event.source || 'api',
    ]);

    return this.mapRow(result.rows[0]);
  }

  /**
   * Get all events for an aggregate
   */
  async getAggregateEvents(aggregateId: string): Promise<EventRecord[]> {
    const result = await this.pool.query(`
      SELECT * FROM events
      WHERE aggregate_id = $1
      ORDER BY version ASC
    `, [aggregateId]);

    return result.rows.map(row => this.mapRow(row));
  }

  /**
   * Get current version of aggregate
   */
  async getAggregateVersion(aggregateId: string): Promise<number | null> {
    const result = await this.pool.query(`
      SELECT MAX(version) as version
      FROM events
      WHERE aggregate_id = $1
    `, [aggregateId]);

    return result.rows[0]?.version || null;
  }

  /**
   * Get events by user (time range)
   */
  async getUserEvents(
    userId: string,
    from: Date,
    to: Date
  ): Promise<EventRecord[]> {
    const result = await this.pool.query(`
      SELECT * FROM events
      WHERE user_id = $1
        AND timestamp >= $2
        AND timestamp <= $3
      ORDER BY timestamp DESC
      LIMIT 1000
    `, [userId, from, to]);

    return result.rows.map(row => this.mapRow(row));
  }

  /**
   * Subscribe to events (polling-based, real-time in EventStoreDB)
   */
  async subscribe(
    callback: (event: EventRecord) => Promise<void>,
    options?: { fromTimestamp?: Date }
  ): Promise<void> {
    const from = options?.fromTimestamp || new Date();
    
    while (true) {
      const result = await this.pool.query(`
        SELECT * FROM events
        WHERE timestamp > $1
        ORDER BY timestamp ASC
        LIMIT 100
      `, [from]);

      for (const row of result.rows) {
        await callback(this.mapRow(row));
      }

      // Wait before polling again
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }

  private mapRow(row: any): EventRecord {
    return {
      id: row.id,
      timestamp: new Date(row.timestamp),
      aggregateId: row.aggregate_id,
      aggregateType: row.aggregate_type,
      eventType: row.event_type,
      userId: row.user_id,
      data: typeof row.data === 'string' ? JSON.parse(row.data) : row.data,
      metadata: row.metadata ? (typeof row.metadata === 'string' ? JSON.parse(row.metadata) : row.metadata) : undefined,
      version: row.version,
      causationId: row.causation_id,
      correlationId: row.correlation_id,
      source: row.source,
    };
  }
}

// Export singleton
const timescalePool = new Pool({ connectionString: process.env.TIMESCALE_URL });
export const eventRepository = new EventRepository(timescalePool);
```

**Time**: 6 hours  
**Deliverable**: EventRepository ready ‚úÖ

---

### Days 9-10 (Thu-Fri): Dual-Write & Migration

#### Day 9: Dual-Write Events

**Code**:
```typescript
// Write to BOTH PostgreSQL AND TimescaleDB
async function logEvent(event) {
  // Write to TimescaleDB
  const timescaleEvent = await eventRepository.append(event);
  
  // Also write to PostgreSQL (old system)
  const pgEvent = await db.insert(events).values(event);
  
  // Verify both succeeded
  if (timescaleEvent.id && pgEvent.id) {
    console.log('‚úÖ Dual-write successful');
  } else {
    console.error('‚ùå Dual-write failed');
  }
  
  return timescaleEvent;
}
```

**Time**: 4 hours

---

#### Day 10: Migrate Historical Events

**Script**:
```typescript
// scripts/migrate-events-to-timescale.ts
async function migrateEventsToTimescale() {
  // Get all events from PostgreSQL
  const pgEvents = await db.select().from(events).all();
  
  console.log(`üìä Migrating ${pgEvents.length} events...`);
  
  for (const event of pgEvents) {
    await eventRepository.append({
      aggregateId: event.id,  // Map to new structure
      aggregateType: 'entity',
      eventType: event.type,
      userId: event.userId,
      data: event.data,
      version: 1,  // Assign versions
      source: event.source,
    });
  }
  
  console.log('‚úÖ Migration complete!');
}
```

**Time**: 4 hours  
**Deliverable**: All events in TimescaleDB ‚úÖ

---

## üóìÔ∏è Week 3: Architecture Cleanup

### Days 11-12 (Mon-Tue): Type System & Enums

**Task**: Create centralized type package

**Structure**:
```
packages/types/
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts
‚îÇ   ‚îú‚îÄ‚îÄ enums.ts        # All enums
‚îÇ   ‚îú‚îÄ‚îÄ branded.ts      # Branded types
‚îÇ   ‚îú‚îÄ‚îÄ events.ts       # Event interfaces
‚îÇ   ‚îú‚îÄ‚îÄ entities.ts     # Entity interfaces
‚îÇ   ‚îî‚îÄ‚îÄ value-objects.ts # Immutable data
```

**Code**:
```typescript
// packages/types/src/enums.ts
export enum AggregateType {
  ENTITY = 'entity',
  RELATION = 'relation',
  USER = 'user',
}

export enum EntityType {
  NOTE = 'note',
  TASK = 'task',
  PROJECT = 'project',
  PAGE = 'page',
  EVENT = 'event',
  IDEA = 'idea',
}

export enum EventType {
  // Entity lifecycle
  ENTITY_CREATED = 'entity.created',
  ENTITY_UPDATED = 'entity.updated',
  ENTITY_DELETED = 'entity.deleted',
  
  // Entity properties
  ENTITY_TITLE_CHANGED = 'entity.title_changed',
  ENTITY_FILE_UPLOADED = 'entity.file_uploaded',
  
  // Task
  TASK_STATUS_CHANGED = 'task.status_changed',
  TASK_COMPLETED = 'task.completed',
  
  // AI
  AI_ANALYSIS_REQUESTED = 'ai.analysis_requested',
  AI_ENRICHMENT_COMPLETED = 'ai.enrichment_completed',
}

export enum FileType {
  MARKDOWN = 'markdown',
  PDF = 'pdf',
  AUDIO = 'audio',
  VIDEO = 'video',
  IMAGE = 'image',
}

export enum TaskStatus {
  TODO = 'todo',
  IN_PROGRESS = 'in_progress',
  DONE = 'done',
  CANCELLED = 'cancelled',
}

export enum EventSource {
  API = 'api',
  AUTOMATION = 'automation',
  SYNC = 'sync',
  MIGRATION = 'migration',
  SYSTEM = 'system',
}

export enum RelationType {
  CONTAINS = 'contains',
  RELATES_TO = 'relates_to',
  BLOCKS = 'blocks',
  DUPLICATES = 'duplicates',
  MENTIONS = 'mentions',
  DERIVED_FROM = 'derived_from',
}
```

**Time**: 8 hours  
**Deliverable**: @synap/types package ‚úÖ

---

### Days 13-14 (Wed-Thu): Remove Inngest Projectors

**Task**: Synchronous projections for simple ops

**Changes**:
```typescript
// packages/api/src/routers/notes.ts
.mutation(async ({ ctx, input }) => {
  const userId = requireUserId(ctx.userId);
  const entityId = randomUUID();
  
  // Transaction: event + projection together
  await db.transaction(async (tx) => {
    // 1. Upload to R2
    const file = await r2.upload(
      R2Storage.buildPath(userId, 'note', entityId),
      input.content
    );
    
    // 2. Append event to TimescaleDB
    await eventRepository.append({
      aggregateId: entityId,
      aggregateType: AggregateType.ENTITY,
      eventType: EventType.ENTITY_CREATED,
      userId,
      data: {
        type: EntityType.NOTE,
        fileUrl: file.url,
        fileSize: file.size,
      },
      version: 1,
    });
    
    // 3. Update projection (synchronous!)
    await tx.insert(entities).values({
      id: entityId,
      userId,
      type: EntityType.NOTE,
      fileUrl: file.url,
      filePath: file.path,
      fileSize: file.size,
      fileType: FileType.MARKDOWN,
      checksum: file.checksum,
      version: 1,
    });
  });
  
  // 4. Async AI enrichment (still use Inngest for this!)
  await inngest.send({
    name: 'ai.enrich_requested',
    data: { entityId, userId, fileUrl: file.url },
  });
  
  return { entityId, fileUrl: file.url };
});
```

**Remove**:
```typescript
// ‚ùå DELETE packages/jobs/src/functions/projectors.ts
// No longer needed - projections are synchronous
```

**Keep**:
```typescript
// ‚úÖ KEEP packages/jobs/src/functions/ai-analyzer.ts
// Complex workflow - needs Inngest
```

**Time**: 8 hours  
**Deliverable**: Projections synchronous ‚úÖ

---

### Day 15 (Friday): Storage Interface

**Task**: Make @initiativ/storage an interface

**Code**:
```typescript
// packages/@initiativ-core/src/interfaces/storage.ts (NEW)
export interface IStorage {
  // Create
  createEntity(data: {
    userId: string;
    type: string;
    content: string;
    title?: string;
    tags?: string[];
  }): Promise<Entity>;
  
  // Read
  getEntity(id: string, userId: string): Promise<Entity>;
  getEntityContent(id: string, userId: string): Promise<string>;
  
  // Update
  updateEntity(id: string, userId: string, data: Partial<Entity>): Promise<Entity>;
  updateEntityContent(id: string, userId: string, content: string): Promise<void>;
  
  // Delete
  deleteEntity(id: string, userId: string): Promise<void>;
  
  // Search
  searchEntities(userId: string, query: string): Promise<Entity[]>;
}

// packages/api/src/storage/postgres-r2-storage.ts (NEW)
export class PostgresR2Storage implements IStorage {
  constructor(
    private db: DrizzleDb,
    private r2: R2Storage,
    private eventRepo: EventRepository
  ) {}

  async createEntity(data): Promise<Entity> {
    const entityId = randomUUID();
    
    // 1. Upload to R2
    const file = await this.r2.upload(
      R2Storage.buildPath(data.userId, data.type, entityId),
      data.content
    );
    
    // 2. Transactional write
    const entity = await this.db.transaction(async (tx) => {
      // Event
      await this.eventRepo.append({
        aggregateId: entityId,
        aggregateType: AggregateType.ENTITY,
        eventType: EventType.ENTITY_CREATED,
        userId: data.userId,
        data: {
          type: data.type,
          title: data.title,
          fileUrl: file.url,
        },
        version: 1,
      });
      
      // Projection
      const [entity] = await tx.insert(entities).values({
        id: entityId,
        userId: data.userId,
        type: data.type,
        title: data.title,
        fileUrl: file.url,
        filePath: file.path,
        fileSize: file.size,
        fileType: FileType.MARKDOWN,
        checksum: file.checksum,
        version: 1,
      }).returning();
      
      return entity;
    });
    
    return entity;
  }

  async getEntityContent(id: string, userId: string): Promise<string> {
    // Get file reference from DB
    const entity = await this.getEntity(id, userId);
    
    if (!entity.filePath) {
      throw new Error('Entity has no file');
    }
    
    // Download from R2
    return await this.r2.download(entity.filePath);
  }
  
  // ... other methods
}

// Update @initiativ/core to use interface
class Workflows {
  constructor(private storage: IStorage) {}  // ‚úÖ Dependency injection
  
  async captureNote(input) {
    return await this.storage.createEntity({
      userId: this.userId,
      type: 'note',
      content: input.data,
    });
  }
}
```

**Time**: 8 hours  
**Deliverable**: Storage abstraction complete ‚úÖ

---

## üìä Daily Progress Tracking

### Week 1 Checklist

```
Day 1:
  [x] R2 bucket created
  [x] R2Storage class implemented
  [x] Unit tests passing
  
Day 2:
  [x] entities schema updated (file fields)
  [x] entity_vectors table created
  [x] Migrations applied
  
Day 3:
  [x] Dual-write implemented
  [x] Monitoring dashboard
  [x] No errors for 24 hours
  
Day 4:
  [x] Migration script tested
  [x] All content migrated to R2
  [x] Verification passed
  
Day 5:
  [x] content_blocks table dropped
  [x] API still works
  [x] Week 1 complete! üéâ
```

### Week 2 Checklist

```
Day 6:
  [x] Neon TimescaleDB support checked
  [x] Decision made (Railway/EventStoreDB/etc)
  [x] Alternative DB deployed
  
Day 7:
  [x] TimescaleDB extension enabled
  [x] Hypertable created
  [x] Compression policy set
  
Day 8:
  [x] EventRepository implemented
  [x] Unit tests passing
  [x] Integration tested
  
Day 9:
  [x] Dual-write events
  [x] Validation passing
  [x] No data loss
  
Day 10:
  [x] Historical events migrated
  [x] PostgreSQL events table dropped
  [x] Week 2 complete! üéâ
```

### Week 3 Checklist

```
Days 11-12:
  [x] @synap/types package created
  [x] All enums defined
  [x] Branded types implemented
  [x] All packages updated
  
Days 13-14:
  [x] Projectors removed
  [x] Sync projections working
  [x] Complex Inngest workflows kept
  [x] Performance validated
  
Day 15:
  [x] IStorage interface created
  [x] PostgresR2Storage implemented
  [x] @initiativ/core updated
  [x] All tests passing
  [x] V0.3 COMPLETE! üéâ
```

---

## üß™ Testing Plan

### Regression Tests (Run Daily)

```bash
# User isolation
npx vitest run packages/core/tests/user-isolation.test.ts

# Note creation
npx vitest run packages/core/tests/note-creation.test.ts

# Event replay
npx vitest run packages/core/tests/event-replay.test.ts
```

### Performance Tests

```bash
# Before migration
npm run benchmark:baseline

# After migration
npm run benchmark:v03

# Compare
npm run benchmark:compare
```

**Metrics to Track**:
- API latency (target: <200ms)
- Event append speed (target: <50ms)
- R2 upload speed (target: <500ms)
- Search performance (target: <300ms)

---

### Load Tests

```javascript
// tests/load/create-notes.js
import { check } from 'k6';
import http from 'k6/http';

export const options = {
  vus: 100, // 100 concurrent users
  duration: '5m',
};

export default function () {
  const res = http.post('http://localhost:3000/trpc/notes.create', {
    content: `Test note ${__VU}-${__ITER}`,
  });
  
  check(res, {
    'status 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
}
```

**Run**:
```bash
k6 run tests/load/create-notes.js
```

**Success Criteria**:
- ‚úÖ 100 concurrent users
- ‚úÖ <500ms p95 latency
- ‚úÖ <1% error rate

---

## üí∞ Budget & Resources

### Infrastructure Costs (Monthly)

```
Cloudflare R2:
- Storage (10TB): $150
- Operations: ~$50
Subtotal: $200/month

TimescaleDB (Railway):
- Database: $5-20/month
Subtotal: $20/month

PostgreSQL (Neon):
- Reduced usage: $100/month
Subtotal: $100/month

Inngest:
- Reduced runs: $25/month
Subtotal: $25/month

Redis (Optional):
- Upstash: $10/month
Subtotal: $10/month

TOTAL: ~$355/month (vs $11,600 current)
SAVINGS: $11,245/month (97%!)
```

---

### Development Time

```
Week 1: 40 hours (R2 migration)
Week 2: 40 hours (TimescaleDB)
Week 3: 40 hours (Cleanup)

Total: 120 hours
Rate: $100/hour (contractor)
Cost: $12,000

ROI: $11,245/month savings
Payback: 1.07 months
```

---

## üéØ Risk Mitigation

### Data Loss Prevention

```
1. Backup before each phase:
   pg_dump $DATABASE_URL > backup-$(date +%Y%m%d).sql

2. Dual-write period (3-7 days):
   - Write to both old and new systems
   - Verify consistency
   - Monitor for discrepancies

3. Rollback plan:
   - Keep old tables for 30 days
   - Can restore if needed
   - Zero data loss guarantee
```

### Performance Validation

```
1. Load testing after each phase
2. Monitor P95 latency
3. Rollback if >20% regression
```

### User Communication

```
1. Maintenance window announcements
2. Status page updates
3. Rollback SLA: <1 hour
```

---

## üöÄ GO/NO-GO Decision Points

### End of Day 3 (Dual-Write)

**Criteria**:
- [ ] R2 uploads working
- [ ] Dual-write successful for 100+ entities
- [ ] Zero errors in logs
- [ ] Performance acceptable

**If NO-GO**: Pause, debug, then resume

---

### End of Day 10 (TimescaleDB)

**Criteria**:
- [ ] All events migrated
- [ ] Query performance improved
- [ ] Zero data loss
- [ ] Compression working

**If NO-GO**: Rollback to PostgreSQL, re-evaluate

---

### End of Week 3 (Final)

**Criteria**:
- [ ] All tests passing
- [ ] No redundancy
- [ ] Cost reduced by >80%
- [ ] Performance improved by >50%

**If NO-GO**: Keep V0.2, debt remains

---

## ‚úÖ Ready to Start?

**Confirm**:
- [ ] Cloudflare account created
- [ ] Budget approved ($12K dev + $355/month infra)
- [ ] Timeline acceptable (3 weeks)
- [ ] Risk understood (medium, mitigated)

**First Command** (After confirmation):
```bash
# Create Cloudflare R2 bucket
# Then run:
pnpm add @aws-sdk/client-s3 @aws-sdk/s3-request-presigner --filter @synap/storage
```

---

**Status**: üü¢ **READY TO EXECUTE**

**Next**: Confirm approval ‚Üí Start Day 1 implementation

